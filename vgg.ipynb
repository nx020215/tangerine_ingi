{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This work employs deep convolution neural network model to distinguish between fresh and rotten products in three popular fruits, apple, orange and banana. An accuracy of 99% was reached with both VGG16 and ResNet101 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:01.280684Z",
     "iopub.status.busy": "2023-04-20T08:52:01.280353Z",
     "iopub.status.idle": "2023-04-20T08:52:01.294569Z",
     "shell.execute_reply": "2023-04-20T08:52:01.293626Z",
     "shell.execute_reply.started": "2023-04-20T08:52:01.280652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:01.301213Z",
     "iopub.status.busy": "2023-04-20T08:52:01.300913Z",
     "iopub.status.idle": "2023-04-20T08:52:15.293066Z",
     "shell.execute_reply": "2023-04-20T08:52:15.291826Z",
     "shell.execute_reply.started": "2023-04-20T08:52:01.301187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imutils in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "# Image processing libraries\n",
    "!pip install imutils\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T20:52:57.522505Z",
     "iopub.status.busy": "2023-04-19T20:52:57.521775Z",
     "iopub.status.idle": "2023-04-19T20:53:07.770627Z",
     "shell.execute_reply": "2023-04-19T20:53:07.769303Z",
     "shell.execute_reply.started": "2023-04-19T20:52:57.522464Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fetching the fresh train and validation data.\n",
    "import os\n",
    "FRESH_PATH = \"C:\\\\Users\\\\USER\\\\Desktop\\\\algo\"\n",
    "FRESH_TRAIN_PATH = os.path.sep.join([FRESH_PATH, \"train\"])\n",
    "FRESH_TEST_PATH = os.path.sep.join([FRESH_PATH, \"test\"])\n",
    "\n",
    "# Get total number of images per class\n",
    "freshTotalTrain = len(list(paths.list_images(FRESH_TRAIN_PATH)))\n",
    "freshTotalTest = len(list(paths.list_images(FRESH_TEST_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T18:21:39.773585Z",
     "iopub.status.busy": "2023-04-19T18:21:39.773132Z",
     "iopub.status.idle": "2023-04-19T18:21:39.781135Z",
     "shell.execute_reply": "2023-04-19T18:21:39.779685Z",
     "shell.execute_reply.started": "2023-04-19T18:21:39.773536Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating the CLAHE Function\n",
    "clahe = cv2.createCLAHE(clipLimit=10,tileGridSize=(8,8))\n",
    "\n",
    "def apply_clahe(img):\n",
    "    img = np.array(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n",
    "    img[:,:,0] = clahe.apply(img[:,:,0])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_Lab2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply CLAHE to all images and save to the destination directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T18:23:12.333584Z",
     "iopub.status.busy": "2023-04-19T18:23:12.332623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image: C:\\Users\\USER\\Desktop\\algo\\dataset\\train\\rotteng\\다운로드 (1).jpg\n",
      "Error loading image: C:\\Users\\USER\\Desktop\\algo\\dataset\\train\\rotteng\\다운로드 (2).jpg\n",
      "Error loading image: C:\\Users\\USER\\Desktop\\algo\\dataset\\train\\rotteng\\다운로드 (3).jpg\n",
      "Error loading image: C:\\Users\\USER\\Desktop\\algo\\dataset\\train\\rotteng\\다운로드 (4).jpg\n",
      "Error loading image: C:\\Users\\USER\\Desktop\\algo\\dataset\\train\\rotteng\\다운로드.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the source directory containing the images\n",
    "source_dir = \"C:\\\\Users\\\\USER\\\\Desktop\\\\algo\\\\dataset\"\n",
    "\n",
    "# Define the destination directory to save the processed images\n",
    "destination_dir = \"C:\\\\Users\\\\USER\\\\Desktop\\\\algo\\\\dataset2\"\n",
    "\n",
    "# Define a function to apply CLAHE on the image\n",
    "def apply_clahe(image):\n",
    "    return processed_image\n",
    "\n",
    "for subdir, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        \n",
    "        if image is not None:\n",
    "            # Apply the custom function to process the image\n",
    "            processed_image = apply_clahe(image)\n",
    "            \n",
    "            # Get the relative path of the image with respect to the source directory\n",
    "            relative_path = os.path.relpath(file_path, source_dir)\n",
    "            \n",
    "            # Construct the destination path to save the processed image\n",
    "            destination_path = os.path.join(destination_dir, relative_path)\n",
    "            \n",
    "            # Create the destination directory if it does not exist\n",
    "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "            \n",
    "            # Save the processed image to the destination directory\n",
    "            cv2.imwrite(destination_path, processed_image)\n",
    "        else:\n",
    "            print(f\"Error loading image: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:15.297601Z",
     "iopub.status.busy": "2023-04-20T08:52:15.297226Z",
     "iopub.status.idle": "2023-04-20T08:52:15.356609Z",
     "shell.execute_reply": "2023-04-20T08:52:15.355567Z",
     "shell.execute_reply.started": "2023-04-20T08:52:15.297566Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fetch CLAHE-converted images\n",
    "import os\n",
    "BASE_PATH = \"C:\\\\Users\\\\USER\\\\Desktop\\\\algo\\\\dataset2\"\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"train\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"test\"])\n",
    "\n",
    "# Get total number of images per class\n",
    "totalTrain = len(list(paths.list_images(TRAIN_PATH)))\n",
    "totalTest = len(list(paths.list_images(TEST_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 images belonging to 2 classes.\n",
      "Found 344 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 이미지 증강을 위한 데이터 생성기 설정\n",
    "trainAug = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# 훈련용 및 검증용 데이터셋 생성\n",
    "train_generator = trainAug.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = trainAug.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    subset=\"validation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show comparison between fresh and CLAHE-applied images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T18:49:58.365399Z",
     "iopub.status.busy": "2023-04-19T18:49:58.364662Z",
     "iopub.status.idle": "2023-04-19T18:55:04.525952Z",
     "shell.execute_reply": "2023-04-19T18:55:04.524750Z",
     "shell.execute_reply.started": "2023-04-19T18:49:58.365360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\clahe-dataset.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('clahe-dataset', 'zip', destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:15.358825Z",
     "iopub.status.busy": "2023-04-20T08:52:15.358434Z",
     "iopub.status.idle": "2023-04-20T08:52:15.370131Z",
     "shell.execute_reply": "2023-04-20T08:52:15.368834Z",
     "shell.execute_reply.started": "2023-04-20T08:52:15.358787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes names:\n",
      ".............\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['freshg', 'rotteng']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting labels of training data\n",
    "labels = os.listdir(TRAIN_PATH)\n",
    "print(\"Classes names:\")\n",
    "print(\".............\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:15.373391Z",
     "iopub.status.busy": "2023-04-20T08:52:15.372995Z",
     "iopub.status.idle": "2023-04-20T08:52:15.379920Z",
     "shell.execute_reply": "2023-04-20T08:52:15.378730Z",
     "shell.execute_reply.started": "2023-04-20T08:52:15.373353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the training data for augmentation \n",
    "trainAug= ImageDataGenerator(\n",
    "    validation_split = 0.2,\n",
    "\trescale=1./255,\n",
    "\trotation_range=25,\n",
    "\tzoom_range=0.2,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.2,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "testAug= ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:24.203313Z",
     "iopub.status.busy": "2023-04-20T08:52:24.202277Z",
     "iopub.status.idle": "2023-04-20T08:52:24.636477Z",
     "shell.execute_reply": "2023-04-20T08:52:24.635355Z",
     "shell.execute_reply.started": "2023-04-20T08:52:24.203234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 images belonging to 2 classes.\n",
      "Found 344 images belonging to 2 classes.\n",
      "Found 332 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# initialize the training generator\n",
    "train_generator = trainAug.flow_from_directory(\n",
    "\tTRAIN_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=32,\n",
    "    subset = \"training\")\n",
    "\n",
    "val_generator = trainAug.flow_from_directory(\n",
    "\tTRAIN_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=32,\n",
    "    subset = \"validation\")\n",
    "\n",
    "\n",
    "# initialize the validation generator\n",
    "test_generator = testAug.flow_from_directory(\n",
    "\tTEST_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the utility model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:28.450906Z",
     "iopub.status.busy": "2023-04-20T08:52:28.449937Z",
     "iopub.status.idle": "2023-04-20T08:52:28.696097Z",
     "shell.execute_reply": "2023-04-20T08:52:28.694830Z",
     "shell.execute_reply.started": "2023-04-20T08:52:28.450867Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def data_viz(model, history):\n",
    "  pyplot.style.use(\"ggplot\")\n",
    "  # sns.set()\n",
    "  fig = pyplot.figure(0, (12, 4))\n",
    "\n",
    "  ax = pyplot.subplot(1, 2, 1)\n",
    "  pyplot.plot(np.arange(0, len(history.epoch)), history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "  pyplot.plot(np.arange(0, len(history.epoch)), history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "  pyplot.title(\"\"+ model + \" Training and Validation Accuracy\")\n",
    "  pyplot.xlabel(\"Epoch #\")\n",
    "  pyplot.ylabel(\"Accuracy\")\n",
    "  pyplot.legend(loc=\"lower right\")\n",
    "  pyplot.title('Accuracy')\n",
    "  pyplot.tight_layout()\n",
    "\n",
    "  ax = pyplot.subplot(1, 2, 2)\n",
    "  pyplot.plot(np.arange(0, len(history.epoch)), history.history[\"loss\"], label=\"train_loss\")\n",
    "  pyplot.plot(np.arange(0, len(history.epoch)), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "  pyplot.title(\"\"+ model + \" Training and Validation Loss\")\n",
    "  pyplot.xlabel(\"Epoch #\")\n",
    "  pyplot.ylabel(\"Loss\")\n",
    "  pyplot.legend(loc=\"upper right\")\n",
    "  pyplot.tight_layout()\n",
    "\n",
    "  pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T20:58:11.488300Z",
     "iopub.status.busy": "2023-04-19T20:58:11.487141Z",
     "iopub.status.idle": "2023-04-19T20:58:11.493646Z",
     "shell.execute_reply": "2023-04-19T20:58:11.492274Z",
     "shell.execute_reply.started": "2023-04-19T20:58:11.488245Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T20:58:11.827766Z",
     "iopub.status.busy": "2023-04-19T20:58:11.826479Z",
     "iopub.status.idle": "2023-04-19T20:58:16.123266Z",
     "shell.execute_reply": "2023-04-19T20:58:16.122074Z",
     "shell.execute_reply.started": "2023-04-19T20:58:11.827709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "n_classes=2\n",
    "conv_base = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T20:58:16.125873Z",
     "iopub.status.busy": "2023-04-19T20:58:16.125445Z",
     "iopub.status.idle": "2023-04-19T20:58:16.132627Z",
     "shell.execute_reply": "2023-04-19T20:58:16.131054Z",
     "shell.execute_reply.started": "2023-04-19T20:58:16.125831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T20:59:03.859667Z",
     "iopub.status.busy": "2023-04-19T20:59:03.858899Z",
     "iopub.status.idle": "2023-04-19T20:59:03.974138Z",
     "shell.execute_reply": "2023-04-19T20:59:03.973311Z",
     "shell.execute_reply.started": "2023-04-19T20:59:03.859624Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21137729 (80.63 MB)\n",
      "Trainable params: 6423041 (24.50 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "# Create inputs with correct shape\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = conv_base(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a hidden layer\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Add a dropout layer\n",
    "x = Dropout(.5)(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = Dense(1, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "vgg_model_clahe = Model(inputs, outputs)\n",
    "vgg_model_clahe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T21:00:07.029759Z",
     "iopub.status.busy": "2023-04-19T21:00:07.028944Z",
     "iopub.status.idle": "2023-04-19T21:43:29.974887Z",
     "shell.execute_reply": "2023-04-19T21:43:29.973761Z",
     "shell.execute_reply.started": "2023-04-19T21:00:07.029715Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/16\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "43/43 [==============================] - ETA: 0s - loss: 1.1425 - accuracy: 0.5000\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to vgg16_model_best_weights.h5\n",
      "43/43 [==============================] - 471s 11s/step - loss: 1.1425 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/16\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "43/43 [==============================] - 465s 11s/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/16\n",
      "15/43 [=========>....................] - ETA: 4:09 - loss: 0.6931 - accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Model weights are saved at the end of every epoch, if it's the best seen so far.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vgg_checkpoint_clahe \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_filepath,save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m vgg_history_clahe \u001b[38;5;241m=\u001b[39m vgg_model_clahe\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      9\u001b[0m     train_generator,\n\u001b[0;32m     10\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(train_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mbatch_size),\n\u001b[0;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     12\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_generator,\n\u001b[0;32m     13\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(val_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m val_generator\u001b[38;5;241m.\u001b[39mbatch_size),\n\u001b[0;32m     14\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[vgg_checkpoint_clahe]\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg_model_clahe.compile(loss='binary_crossentropy',\n",
    "                         metrics=[\"accuracy\"],\n",
    "                         optimizer='nadam')\n",
    "checkpoint_filepath = 'vgg16_model_best_weights.h5'\n",
    "# Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "vgg_checkpoint_clahe = ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True,verbose=1)\n",
    "\n",
    "vgg_history_clahe = vgg_model_clahe.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=int(train_generator.samples / train_generator.batch_size),\n",
    "    epochs=16,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=int(val_generator.samples / val_generator.batch_size),\n",
    "    callbacks=[vgg_checkpoint_clahe]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T21:46:30.914681Z",
     "iopub.status.busy": "2023-04-19T21:46:30.913934Z",
     "iopub.status.idle": "2023-04-19T21:46:52.040016Z",
     "shell.execute_reply": "2023-04-19T21:46:52.038897Z",
     "shell.execute_reply.started": "2023-04-19T21:46:30.914638Z"
    }
   },
   "outputs": [],
   "source": [
    "data_viz(\"VGG\", vgg_history_clahe)\n",
    "vgg_loss, vgg_accuracy = vgg_model_clahe.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T21:48:26.707836Z",
     "iopub.status.busy": "2023-04-19T21:48:26.707448Z",
     "iopub.status.idle": "2023-04-19T21:48:40.891345Z",
     "shell.execute_reply": "2023-04-19T21:48:40.889934Z",
     "shell.execute_reply.started": "2023-04-19T21:48:26.707801Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = vgg_model_clahe.predict(test_generator, steps=test_generator.samples/test_generator.batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['freshg', 'rotteng']\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T21:49:00.695916Z",
     "iopub.status.busy": "2023-04-19T21:49:00.695210Z",
     "iopub.status.idle": "2023-04-19T21:49:01.038448Z",
     "shell.execute_reply": "2023-04-19T21:49:01.037061Z",
     "shell.execute_reply.started": "2023-04-19T21:49:00.695877Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_model_clahe.save('/kaggle/working/'+checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T22:11:41.711596Z",
     "iopub.status.busy": "2023-04-19T22:11:41.710789Z",
     "iopub.status.idle": "2023-04-19T22:11:41.716474Z",
     "shell.execute_reply": "2023-04-19T22:11:41.715281Z",
     "shell.execute_reply.started": "2023-04-19T22:11:41.711551Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T22:11:57.870788Z",
     "iopub.status.busy": "2023-04-19T22:11:57.869849Z",
     "iopub.status.idle": "2023-04-19T22:12:02.333732Z",
     "shell.execute_reply": "2023-04-19T22:12:02.332677Z",
     "shell.execute_reply.started": "2023-04-19T22:11:57.870746Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "n_classes=2\n",
    "resnet_base = ResNet101V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T22:12:03.983043Z",
     "iopub.status.busy": "2023-04-19T22:12:03.982399Z",
     "iopub.status.idle": "2023-04-19T22:12:04.001101Z",
     "shell.execute_reply": "2023-04-19T22:12:04.000003Z",
     "shell.execute_reply.started": "2023-04-19T22:12:03.982998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "resnet_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T22:12:05.633315Z",
     "iopub.status.busy": "2023-04-19T22:12:05.632372Z",
     "iopub.status.idle": "2023-04-19T22:12:06.530331Z",
     "shell.execute_reply": "2023-04-19T22:12:06.529282Z",
     "shell.execute_reply.started": "2023-04-19T22:12:05.633260Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "# Create inputs with correct shape\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = resnet_base(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a hidden layer\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Add a dropout layer\n",
    "x = Dropout(.5)(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "resnet_model = Model(inputs, outputs)\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T22:12:18.022052Z",
     "iopub.status.busy": "2023-04-19T22:12:18.021659Z",
     "iopub.status.idle": "2023-04-19T22:56:44.896307Z",
     "shell.execute_reply": "2023-04-19T22:56:44.894578Z",
     "shell.execute_reply.started": "2023-04-19T22:12:18.022010Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet_model.compile(loss='binary_crossentropy',\n",
    "                     metrics=[\"accuracy\"],\n",
    "                     optimizer='nadam')\n",
    "\n",
    "resnet_checkpoint_filepath = 'resnet_model_best_weights.h5'\n",
    "# Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "resnet_checkpoint = ModelCheckpoint(filepath=resnet_checkpoint_filepath,save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True,verbose=1)\n",
    "\n",
    "resnet_history = resnet_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples / train_generator.batch_size,\n",
    "    epochs=16,\n",
    "    validation_steps=val_generator.samples / val_generator.batch_size,\n",
    "    callbacks=[resnet_checkpoint]\n",
    ")\n",
    ")\n",
    "\n",
    "data_viz(\"ResNet50\", resnet_history)\n",
    "resnet_loss, resnet_accuracy = resnet_model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size)\n",
    "resnet_model.save('/kaggle/working/'+resnet_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T23:05:45.483587Z",
     "iopub.status.busy": "2023-04-19T23:05:45.482685Z",
     "iopub.status.idle": "2023-04-19T23:06:02.012444Z",
     "shell.execute_reply": "2023-04-19T23:06:02.011290Z",
     "shell.execute_reply.started": "2023-04-19T23:05:45.483545Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = resnet_model.predict(test_generator, steps=test_generator.samples/test_generator.batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['freshg', 'rotteng']\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T08:52:53.983964Z",
     "iopub.status.busy": "2023-04-20T08:52:53.983463Z",
     "iopub.status.idle": "2023-04-20T08:52:57.065779Z",
     "shell.execute_reply": "2023-04-20T08:52:57.064937Z",
     "shell.execute_reply.started": "2023-04-20T08:52:53.983918Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "input = Input(shape=(224,224,3)) \n",
    "\n",
    "x  = Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(input)\n",
    "x  = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x  = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x  = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x  = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x  = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "x  = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a hidden layer\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Add a dropout layer\n",
    "x = Dropout(.5)(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = Dense(2, activation='softmax')(x)\n",
    "cnn_model = Model(input, outputs)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T11:14:21.145964Z",
     "iopub.status.busy": "2023-04-20T11:14:21.145313Z",
     "iopub.status.idle": "2023-04-20T11:14:21.575566Z",
     "shell.execute_reply": "2023-04-20T11:14:21.574365Z",
     "shell.execute_reply.started": "2023-04-20T11:14:21.145923Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "dot_img_file = 'C:\\\\Users\\\\siwon\\\\Desktop\\\\algo\\\\model_1.png'\n",
    "plot_model(cnn_model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T09:53:52.684156Z",
     "iopub.status.busy": "2023-04-20T09:53:52.683632Z",
     "iopub.status.idle": "2023-04-20T10:36:18.479845Z",
     "shell.execute_reply": "2023-04-20T10:36:18.478584Z",
     "shell.execute_reply.started": "2023-04-20T09:53:52.684111Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer='nadam')\n",
    "\n",
    "cnn_checkpoint_filepath = 'cnn_model_best_weights.h5'\n",
    "# Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "cnn_checkpoint = ModelCheckpoint(filepath=cnn_checkpoint_filepath,\n",
    "                                 save_weights_only=True,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True,\n",
    "                                 verbose=1)\n",
    "\n",
    "# Convert steps_per_epoch and validation_steps to integers\n",
    "steps_per_epoch = int(train_generator.samples / train_generator.batch_size)\n",
    "validation_steps = int(val_generator.samples / val_generator.batch_size)\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=int(train_generator.samples / train_generator.batch_size),\n",
    "    epochs=16,\n",
    "    validation_steps=int(val_generator.samples / val_generator.batch_size),\n",
    "    callbacks=[cnn_checkpoint]\n",
    ")\n",
    "\n",
    "data_viz(\"CNN\", cnn_history)\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size)\n",
    "cnn_model.save('/kaggle/working/'+cnn_checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T10:37:51.475922Z",
     "iopub.status.busy": "2023-04-20T10:37:51.474776Z",
     "iopub.status.idle": "2023-04-20T10:38:05.242370Z",
     "shell.execute_reply": "2023-04-20T10:38:05.241092Z",
     "shell.execute_reply.started": "2023-04-20T10:37:51.475876Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = cnn_model.predict(test_generator, steps=test_generator.samples/test_generator.batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['freshg', 'rotteng']\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T10:43:21.014538Z",
     "iopub.status.busy": "2023-04-20T10:43:21.013523Z",
     "iopub.status.idle": "2023-04-20T10:43:21.569858Z",
     "shell.execute_reply": "2023-04-20T10:43:21.568360Z",
     "shell.execute_reply.started": "2023-04-20T10:43:21.014493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Graph showing accuracies and losses of the three models\n",
    "pyplot.style.use(\"ggplot\")\n",
    "# sns.set()\n",
    "fig = pyplot.figure(0, (12, 4))\n",
    "\n",
    "ax = pyplot.subplot(1, 2, 1)\n",
    "pyplot.plot(np.arange(0, len(vgg_history_clahe.epoch)), vgg_history_clahe.history[\"val_accuracy\"], label=\"vgg16_accuracy\")\n",
    "pyplot.plot(np.arange(0, len(resnet_history.epoch)), resnet_history.history[\"val_accuracy\"], label=\"resnet_accuracy\")\n",
    "pyplot.plot(np.arange(0, len(cnn_history.epoch)), cnn_history.history[\"val_accuracy\"], label=\"cnn_accuracy\")\n",
    "pyplot.title(\"Models Test Accuracies\")\n",
    "pyplot.xlabel(\"Epoch #\")\n",
    "pyplot.ylabel(\"Accuracy\")\n",
    "pyplot.legend(loc=\"lower right\")\n",
    "pyplot.tight_layout()\n",
    "\n",
    "ax = pyplot.subplot(1, 2, 2)\n",
    "pyplot.plot(np.arange(0, len(vgg_history_clahe.epoch)), vgg_history_clahe.history[\"val_loss\"], label=\"vgg16_loss\")\n",
    "pyplot.plot(np.arange(0, len(resnet_history.epoch)), resnet_history.history[\"val_loss\"], label=\"resnet_loss\")\n",
    "pyplot.plot(np.arange(0, len(cnn_history.epoch)), cnn_history.history[\"val_loss\"], label=\"cnn_loss\")\n",
    "pyplot.title(\"Models Test Losses\")\n",
    "pyplot.xlabel(\"Epoch #\")\n",
    "pyplot.ylabel(\"Loss\")\n",
    "pyplot.legend(loc=\"upper right\")\n",
    "pyplot.tight_layout()\n",
    "\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
